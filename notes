- To use default db for queue run:
php artisan queue:table
php artisan migrate
QUEUE_CONNECTION=database

We must start the worker:
php artisan queue:work (run from inside laradock)
* it will now scan the db to look for jobs and will start processing them.
* a worker will execute jobs one by one which can take a long time. However we can run multiple queues at tthe same time. Queues can be given
priority and specific job types can be assigned to a specific queue:
- php artisan queue:work --queue=payments,default (payments has priority because it is first)
- \App\Jobs\ProcessPayment::dispatch()->onQueue('payments');

- $timeout can be used to ensure jobs don't hang if, for wxample, a third party api is unresponsive
- use retryUntil for a time based retry. it will retry repeatedly until time expires.
- $backoff = 2 - waits for 2 secs between each retry

* if a job exhausts all if it's retries laravel stores the failed job in the db. we can grab the uuid and manually retry using pph artisan queue:retry {uuid}
* using the return $this->release() in handle will release job back to the queue to be retried if, for example, a api call has hit a rate limit
the number of $tries will determine how many release() retries are allowed.
* laravel counts an attempt as a retry whether the job was retried as a result of a release() or an exception. we can tell laravel
to count them separately by using the $maxExceptions prop.
* the failed() ethod is run when a job failed.

* job workflows are sequential jobs where subsequent jobs will not execute until the previous one has completed
* batches are non-sequetial batches:
- php artisan queue:batches-table (then migrate)
- add Batchable trait to the job classes

* if a batch job is failed the rest of the jobs in the batch will be cancelled. to stop this behaviour we can use the
allowFailures() method

* a race condition is where two processes try to make changes to the same resource and the same time. We can use Lock functions
of laravel to lock the logic inside the closure

* WithoutOverlapping will release jobs back or they will fail if they exceed the max amount. jobs will be placed back in queue even if there
are multiple instances of the same job in the queue whereupon they will then be executed sequentially.
* To ensure that there is only ever one isntance of the same job in the queue we can implement the ShouldBeUnique on the class
*ShouldBeUniqueUntilProcessing interface can prevent dispatching until the job has started processing.

* the more dependencies we pass to the job the more overhead will be created. this can have implications on high volume
applications. the deps have to be serialized and deserialized. it can consume a lot of resources. we can use app()->make('deployer')
to inject dependencies irectly into the job handler without it having to be serialized etc when it is passed via the constructor of
the job.

* we can pass a dependency directly to the constructor of the job to make sure the job gets the latest state of the dep at the time
at which the job was generated, not when it was picked up by the worker.

* we can use the ShouldBeEncypted interface to get laravel to encrypt the pay load of the job
